I"¤<p>As mentioned in a <!-- raw HTML omitted -->previous<!-- raw HTML omitted --> post, <!-- raw HTML omitted -->nnetsauce<!-- raw HTML omitted --> is a Python package for Statistical/Machine learning and deep learning, based on combinations of <!-- raw HTML omitted -->neural<!-- raw HTML omitted --> networks layers. It could be used for solving regression, classification and multivariate time series forecasting problems. This post makes a more detailed introduction of <code>nnetsauce</code>, with a few examples based on classification and deep learning.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>git clone https://github.com/thierrymoudiki/nnetsauce.git
cd nnetsauce
python setup.py install</p>
<p>`
<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code># 0 - Packages ----- 

# Importing the packages that will be used in the demo
import nnetsauce as ns
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split


# 1 - Datasets -----

# Loading breast cancer data
breast_cancer = datasets.load_breast_cancer()
Z = breast_cancer.data
t = breast_cancer.target


# 2 - Data splitting -----            

# Separating the data into training/testing set, and 
# a validation set
Z_train, Z_test, t_train, t_test = train_test_split(
    Z, t, test_size=0.2, random_state=42)


# 3 - Logistic regression -----

# Fitting the Logistic regression model on 
# training set
regr = linear_model.LogisticRegression()                        
regr.fit(Z_train, t_train)

# predictive accuracy of the model on test set
regr.score(Z_test, t_test)  
</code></pre>
<p>`
<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code># Grid search ---
from sklearn.model_selection import GridSearchCV
# grid search for finding better hyperparameters
np.random.seed(123)
clf = GridSearchCV(cv = 3, estimator = fit_obj,
                   param_grid={'n_hidden_features': range(5, 25), 
                               'row_sample': [0.7,0.8, 0.9], 
                               'dropout': [0.7, 0.8, 0.9], 
                               'n_clusters': [0, 2, 3, 4]}, 
                               verbose=2)

# fitting the model
clf.fit(Z_train, t_train)

# 'best' hyperparameters found 
print(clf.best_params_)
print(clf.best_score_)

# predictive accuracy on test set
clf.best_estimator_.score(Z_test, t_test)
</code></pre>
<p>`
<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>import nnetsauce as ns
from sklearn import datasets, metrics

diabetes = datasets.load_diabetes()
X = diabetes.data 
y = diabetes.target

# layer 1 (base layer) ----
layer1_regr = linear_model.BayesianRidge()
layer1_regr.fit(X[0:100,:], y[0:100])
# RMSE score
np.sqrt(metrics.mean_squared_error(y[100:125], layer1_regr.predict(X[100:125,:])))


# layer 2 using layer 1 ----
layer2_regr = ns.CustomRegressor(obj = layer1_regr, n_hidden_features=3, 
                        direct_link=True, bias=True, 
                        nodes_sim='sobol', activation_name='tanh', 
                        n_clusters=2)
layer2_regr.fit(X[0:100,:], y[0:100])


# RMSE score
np.sqrt(layer2_regr.score(X[100:125,:], y[100:125]))

# layer 3 using layer 2 ----
layer3_regr = ns.CustomRegressor(obj = layer2_regr, n_hidden_features=5, 
            direct_link=True, bias=True, 
            nodes_sim='hammersley', activation_name='sigmoid', 
            n_clusters=2)
layer3_regr.fit(X[0:100,:], y[0:100])

# RMSE score
np.sqrt(layer3_regr.score(X[100:125,:], y[100:125]))
</code></pre>
<p>`
<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
:ET