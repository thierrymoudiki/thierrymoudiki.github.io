---
layout: post
title: "Generalized nonlinear models in nnetsauce"
description: Generalized nonlinear models in nnetsauce
date: 2020-11-28
categories: [ExplainableML, Python]
---

This post explores a few configurations of nnetsauce's Generalized nonlinear models (GNLMs), especially the regularization parameters guarding the model against overfitting. There are many, many other configurations that can be envisaged, which I will do 
 over time. GNLM is still very _young_ and experimental. There'll be no advanced tuning in this post, but rather an analysis of 
some hyperparameters, when everything else is held constant (default hyperparameters). 
Many other examples of use of nnetsauce's GNLM can be found in the following notebook: [thierrymoudiki_040920_examples.ipynb](https://github.com/Techtonique/nnetsauce/blob/master/nnetsauce/demo/thierrymoudiki_040920_examples.ipynb).



Let _X_ be a matrix of explanatory variables for a response _y_. The general philosophy 
of GNLMs in the [nnetsauce](https://github.com/Techtonique/nnetsauce) is to 
minimize a loss function _L_ defined as:  

$$ 
\begin{eqnarray*}
    L  &=& loss(y, Z; \beta)\\ 
    &+& \lambda_1 \left( \alpha_1||\beta_1||_1 + \frac{1}{2}(1 - \alpha_1)||\beta_1||_2^2 \right)\\ 
    &+& \lambda_2 \left( \alpha_2||\beta_2||_1 + \frac{1}{2}(1 - \alpha_2)||\beta_2||_2^2 \right)\\ 
\end{eqnarray*}
$$

_Z_ is a transformed version of _X_; a columnwise concatenation of a standardized _X_ and _g(XW+b)_. _g(XW+b)_? 

- _g_ is an elementwise activation function, for example $$x \mapsto max(x, 0)$$, which makes the whole learning procedure nonlinear. 
- _W_ is drawn from a deterministic Sobol sequence; it helps in creating new, various features from _X_. 
- _b_ is a bias term. 
- $$\lambda$$ and $$\alpha$$ both create [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization)-like penalties on model coefficients $$\beta$$. Typically, they add some bias to the basis loss (_loss_), so that the GNLMs are able to generalize when dealing with  unseen data.

Regarding $$\beta$$ now, some of its coefficients are related to _X_ ($$\beta_1$$), and the rest to _g(XW+b)_ ($$\beta_2$$). Examples of $$loss$$ functions include (non-exhaustive list): 

- Gaussian: 

$$
loss(y, Z; \beta) = \frac{1}{2} || y - Z \beta ||_2^2
$$

- Laplace

$$
loss(y, Z; \beta) = || y - Z \beta  ||_1
$$

Ultimately, nnetsauce's implementation of GNLMs will include other loss functions such as binomial or Poisson likelihoods (...) for count data. 


Minimizing _L_ is currently achieved in the [nnetsauce](https://github.com/Techtonique/nnetsauce) by employing [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) and/or __stochastic [coordinate descent](https://en.wikipedia.org/wiki/Coordinate_descent)__ (SCD) (there are certainly other possibilities/choices). In this post, we will use the latter,   which is less encountered in the _wild_. nnetsauce's implementation of SCD can use only a subsample of the whole dataset at each iteration of the optimizer. A stratified subsampling of _y_ is applied, so that the distribution of _y_'s subsamples is very close to the distribution of the whole _y_. 

**5-fold cross-validation RMSE as function of $$\lambda_1$$ and $$\lambda_2$$ on 100 observations (California Housing data)**

x-axis: $$\lambda_2$$, y-axis:  $$\lambda_1$$; both ranging from 10 to 1e-5 (7 points)

![pres-image]({{base}}/images/2020-11-28/2020-11-28-image1.png){:class="img-responsive"}

**5-fold cross-validation RMSE as function of $$\alpha_1$$ and $$\alpha_2$$ on 100 observations (California Housing data)**

x-axis: $$\alpha_2$$, y-axis: $$\alpha_1$$; both ranging from 0 to 1 (5 points)

![pres-image]({{base}}/images/2020-11-28/2020-11-28-image2.png){:class="img-responsive"}

All else held constant, to achieve a _good_ performance **on this dataset**, the model prefers relatively low values of $$\lambda_1$$ and $$\lambda_2$$. When it comes to choosing $$\alpha$$'s, the following _L_ is preferred: 

$$ 
\begin{equation}
    L  = loss(y, Z; \beta)
    + \lambda_1 \left(  \frac{1}{2}||\beta_1||_2^2 \right)
    + \lambda_2 \left( ||\beta_2||_1  \right)
\end{equation}
$$

Remember, though, that there are several other interactions with other hyperparameters. So that, it's better to tune all of them simultaneously.

The code for these graphs (and more) can be found in [this notebook](https://github.com/Techtonique/nnetsauce/blob/master/nnetsauce/demo/thierrymoudiki_271120_glm_with_scd.ipynb).
