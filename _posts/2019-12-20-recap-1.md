---
layout: post
title: "2019 Recap, the nnetsauce, the teller and the querier"
description: The nnetsauce, the teller and the querier; three tools for Statistical Machine Learning
date: 2019-12-20
categories: [DataBases, QuasiRandomizedNN, ExplainableML]
---

This post is a **_recap_ of 2019**. It's especially about the querier, the nnetsauce and the teller. If you want a summary (or a reminder!) of how these tools can be beneficial to you, this is the perfect place. 

# The querier

The [querier](https://github.com/Techtonique/querier) is a __query language__ which helps you to retrieve data from Python `pandas` Data Frames. This language is inspired from Structured Query Language ([SQL](https://en.wikipedia.org/wiki/SQL))'s logic for relational databases. There are currently __9 types of operations__ available in the querier -- with no plan to expand the list much further, to maintain a relatively simple mental model.

In [this post from October 25]({% post_url 2019-10-25-the-querier-1 %}) we present the querier and different verbs constituting its grammar for wrangling data: [`concat`](https://github.com/Techtonique/querier/tree/master/querier/demo/thierrymoudiki_251019_concat.ipynb), [`delete`](https://github.com/Techtonique/querier/tree/master/querier/demo/thierrymoudiki_241019_delete.ipynb), [`drop`](https://github.com/Techtonique/querier/tree/master/querier/demo/thierrymoudiki_241019_drop.ipynb), [`filtr`](https://github.com/Techtonique/querier/tree/master/querier/demo/thierrymoudiki_231019_filtr.ipynb), [`join`](https://github.com/Techtonique/querier/tree/master/querier/demo/thierrymoudiki_231019_join.ipynb), [`select`](https://github.com/Techtonique/querier/tree/master/querier/demo/thierrymoudiki_231019_select.ipynb), [`summarize`](https://github.com/Techtonique/querier/tree/master/querier/demo/thierrymoudiki_231019_summarize.ipynb), [`update`](https://github.com/Techtonique/querier/tree/master/querier/demo/thierrymoudiki_251019_update.ipynb), [`request`](https://github.com/Techtonique/querier/tree/master/querier/demo/thierrymoudiki_231019_request.ipynb). 
 

In [this post from November 22]({% post_url 2019-11-22-the-querier-2 %}), we show how our querier verbs can be __composed__ to form __efficient data wrangling pipelines__. Pipelines? For example: select columns, then filter on rows based on given criteria, and finally, obtain column averages.

In [this post from November 29]({% post_url 2019-11-29-the-querier-3 %}), we examine the querier's performance (speed) on datasets with increasing sizes (up to 1 million rows and 100 columns). The post gives you an idea of what you can expect from the querier when using it on your data. 

# The nnetsauce

The [nnetsauce](https://github.com/Techtonique/nnetsauce) is a __Statistical/Machine learning tool__,  in which pattern recognition is achieved by combining layers of (randomized and) quasi-randomized networks. These building blocks -- layers --  constitute the basis of many custom models, including models with deeper learning architectures for regression, classification, and multivariate time series forecasting. The [following page](https://thierrymoudiki.github.io/software/nnetsauce/index.html) illustrates different use-cases for the nnetsauce, including deep learning application examples. 

This [post from September 18]({% post_url 2019-09-18-nnetsauce-adaboost-1 %}) is about an [Adaptive Boosting](https://en.wikipedia.org/wiki/AdaBoost) (boosting) algorithm variant available in the nnetsauce. This other [post from September 25]({% post_url 2019-09-25-nnetsauce-randombag-1 %}) presents a [Bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating) (bagging) algorithm variant also available in the nnetsauce, and talks about recognizing tomatoes and apples. The main strength of the nnetsauce here on bagging and boosting variants, is to heavily take advantage of randomization to increase ensembles' diversity and accuracy. 

On [October 18]({% post_url 2019-10-18-nnetsauce-prediction-intervals %}), we presented 3 ways for measuring the uncertainty around nnetsauce model predictions: using a __bayesian linear model__ , using a uniform distribution for the network's hidden layers, and using [__dropout__](https://en.wikipedia.org/wiki/Dropout_(neural_networks)) regularization technique. 



# The teller

The [teller](https://github.com/Techtonique/teller) is a __model-agnostic tool for Statistical Machine Learning (ML) explainability__. It uses numerical derivatives to obtain insights on the influence of explanatory variables on a response variable (variable to be explained). 

[This post from November 1]({% post_url 2019-11-01-the-teller-1 %}) introduces the teller's philosophy: a little increase in model's explanatory variables + a little decrease, and we can obtain approximate sensitivities of model predictions to changes in the explanatory variables. Some ML models are accurate, but are considered to be hard to explain (black boxes, relatively to the intuitive linear models). We do not want to sacrifice this high model accuracy to explainability, hence: the teller. 

In [this post from November 8]({% post_url 2019-11-08-the-teller-2 %}), we use [Jackknife](https://en.m.wikipedia.org/wiki/Jackknife_resampling) resampling to obtain confidence intervals around explanatory variables' marginal effects. This resampling procedure allows us to derive confidence intervals and hypotheses tests for the significance of marginal effects (yes, I know that some of you do not like p-values). With these tests, we can identify _important_ explanatory variables (not in the sense of _causation_, though).

[This post from November 15]({% post_url 2019-11-15-the-teller-3 %}) uses the teller to compare two ML models on the Boston Housing dataset; Extremely Randomized  Trees and Random Forest Regressions. By using the teller, we can compare model residuals (model predicted values minus observed _real_ values), their [Multiple R-Squared](https://en.wikipedia.org/wiki/Coefficient_of_determination) and their respective marginal effects side and side. 

 [This post from December 6]({% post_url 2019-12-06-the-teller-4 %}) examines model interactions. By _interactions_, we mean: how does the response variable (variable to be explained) changes when both explanatory variable 1 increases of 1, and  explanatory variable 2 increases of 1. On the Boston Housing dataset used in this post, it means for example: understanding how median value of owner-occupied homes (in $1000’s) changes when the index of accessibility to radial highways and the number of rooms per dwelling increase of 1 __simultaneously__. 

# Conclusion

There's certainly at least one of these tools that excites your curiosity: __comments/remarks/contributions are welcome as usual__, either on Github or _via_ email. 


__This is my last post of 2019, happy holidays!__


__Note:__ I am currently looking for a _gig_. You can hire me on [Malt](https://www.malt.fr/profile/thierrymoudiki) or send me an email: __thierry dot moudiki at pm dot me__. I can do descriptive statistics, data preparation, feature engineering, model calibration, training and validation, and model outputs' interpretation. I am fluent in Python, R, SQL, Microsoft Excel, Visual Basic (among others) and French. My résumé? [Here]({{base}}/cv/thierry-moudiki.pdf)!



