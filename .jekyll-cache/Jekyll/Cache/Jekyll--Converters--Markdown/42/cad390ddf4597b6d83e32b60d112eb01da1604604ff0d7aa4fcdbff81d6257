I"Ë<p>As mentioned in a <a href="/blog/2019/03/13/nnetsauce">previous</a> post, <a href="https://github.com/thierrymoudiki/nnetsauce"><code>nnetsauce</code></a> is a Python package for Statistical/Machine learning and deep learning, based on combinations of <em>neural</em> networks layers. It could be used for solving regression, classification and multivariate time series forecasting problems. This post makes a more detailed introduction of <code>nnetsauce</code>, with a few examples based on classification and deep learning.</p>
<h2>Installing the package</h2>
<p>Currently, <code>nnetsauce</code> can be installed through <a href="https://github.com/thierrymoudiki/nnetsauce">Github</a> (but it will be available on PyPi in a few weeks).</p>
<p>Here is how:</p>
<pre><code class="language-python">git clone https://github.com/thierrymoudiki/nnetsauce.git
cd nnetsauce
python setup.py install
</code></pre>
<h2>Examples of use of <code>nnetsauce</code></h2>
<p>Below, are two examples of use of <a href="https://github.com/thierrymoudiki/nnetsauce"><code>nnetsauce</code></a>. A <strong>classification</strong> example based on breast cancer data, and an illustrative <strong>deep learning</strong> example. In the classification example, we show how a logistic regression model can be enhanced, for a higher accuracy (accuracy is used here for simplicity), by using <code>nnetsauce</code>. The deep learning example shows how custom building blocks of <code>nnetsauce</code> objects can be combined together, to form a - perfectible - deeper learning architecture.</p>
<p><code>scikit-learn</code> models are heavily used in these examples, but <code>nnetsauce</code> <strong>will work with any learning model possessing methods <code>fit()</code> and <code>predict()</code></strong> (plus, <code>predict_proba()</code> for a classifier). That is, it could be used in conjunction with <!-- raw HTML omitted -->xgboost<!-- raw HTML omitted -->, <!-- raw HTML omitted -->LightGBM<!-- raw HTML omitted --> or <!-- raw HTML omitted -->CatBoost<!-- raw HTML omitted --> for example. For the purpose of <strong>model validation</strong>, <code>sklearn</code>'s  cross-validation functions such as <code>GridSearchCV</code> and <code>cross_val_score</code> can be employed (on <code>nnetsauce</code> models), as it will be shown in the classification example.</p>
<h2>Classification example</h2>
<p>For this first example, we start by <strong>fitting a logistic regression model to breast cancer data</strong> on a training set, and measure its accuracy on a validation set:</p>
<pre><code class="language-python">    # 0 - Packages ----- 

    # Importing the packages that will be used in the demo
    import nnetsauce as ns
    from sklearn import datasets, linear_model
    from sklearn.model_selection import train_test_split
    

    # 1 - Datasets -----

    # Loading breast cancer data
    breast_cancer = datasets.load_breast_cancer()
    Z = breast_cancer.data
    t = breast_cancer.target

    
    # 2 - Data splitting -----            

    # Separating the data into training/testing set, and 
    # a validation set
    Z_train, Z_test, t_train, t_test = train_test_split(
        Z, t, test_size=0.2, random_state=42)


    # 3 - Logistic regression -----

    # Fitting the Logistic regression model on 
    # training set
    regr = linear_model.LogisticRegression()                        
    regr.fit(Z_train, t_train)

    # predictive accuracy of the model on test set
    regr.score(Z_test, t_test)  
</code></pre>
<p>The accuracy of this model is equal to <code>0.9561</code>. The <strong>logistic regression is now augmented of <code>n_hidden_features</code> additional features</strong> with <code>nnetsauce</code>. We use <code>GridSearchCV</code> to find a better combination of hyperparameters;  additional hyperparameters such as row subsampling (<code>row_sample</code>) and <code>dropout</code> are included and reseached:<!-- raw HTML omitted -->.</p>
<pre><code class="language-python">    # Defining nnetsauce model
    # based on the logistic regression model
    # defined previously
    fit_obj = ns.CustomClassifier(
    obj=regr,
    n_hidden_features=10,
    direct_link=True,
    bias=True,
    nodes_sim=&quot;sobol&quot;,
    activation_name=&quot;relu&quot;, 
    seed = 123)
    
    # Grid search ---
    from sklearn.model_selection import GridSearchCV
    # grid search for finding better hyperparameters
    np.random.seed(123)
    clf = GridSearchCV(cv = 3, estimator = fit_obj,
                       param_grid={'n_hidden_features': range(5, 25), 
                                   'row_sample': [0.7,0.8, 0.9], 
                                   'dropout': [0.7, 0.8, 0.9], 
                                   'n_clusters': [0, 2, 3, 4]}, 
                                   verbose=2)
    
    # fitting the model
    clf.fit(Z_train, t_train)

    # 'best' hyperparameters found 
    print(clf.best_params_)
    print(clf.best_score_)

    # predictive accuracy on test set
    clf.best_estimator_.score(Z_test, t_test)
</code></pre>
<p>After using <code>nnetsauce</code>, the accuracy is now equal to <code>0.9692</code>.<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>This second example, is an <strong>illustrative</strong> example of deep learning with <!-- raw HTML omitted --><code>nnetsauce</code><!-- raw HTML omitted -->. Many, <strong>more advanced things could be tried</strong>. In this example, predictive accuracy of the model <strong>increases as new layers are added</strong> to the stack.</p>
<!-- raw HTML omitted -->
<p>The <strong>first layer</strong> is a Bayesian ridge regression. Model accuracy (Root Mean Squared Error, RMSE) is equal to <code>63.56</code>. The <strong>second layer</strong> notably uses 3 additional features, an hyperbolic tangent activation function and the first layer; accuracy is <code>61.76</code>. To finish, the <strong>third layer</strong> uses 5 additional features, a sigmoid activation function and the second layer. The final accuracy, after adding this third layer is equal to: <code>61.68</code>.</p>
<!-- raw HTML omitted -->
<pre><code class="language-python">    import nnetsauce as ns
    from sklearn import datasets, metrics

    diabetes = datasets.load_diabetes()
    X = diabetes.data 
    y = diabetes.target
    
    # layer 1 (base layer) ----
    layer1_regr = linear_model.BayesianRidge()
    layer1_regr.fit(X[0:100,:], y[0:100])
    # RMSE score
    np.sqrt(metrics.mean_squared_error(y[100:125], layer1_regr.predict(X[100:125,:])))


    # layer 2 using layer 1 ----
    layer2_regr = ns.CustomRegressor(obj = layer1_regr, n_hidden_features=3, 
                            direct_link=True, bias=True, 
                            nodes_sim='sobol', activation_name='tanh', 
                            n_clusters=2)
    layer2_regr.fit(X[0:100,:], y[0:100])


    # RMSE score
    np.sqrt(layer2_regr.score(X[100:125,:], y[100:125]))

    # layer 3 using layer 2 ----
    layer3_regr = ns.CustomRegressor(obj = layer2_regr, n_hidden_features=5, 
                direct_link=True, bias=True, 
                nodes_sim='hammersley', activation_name='sigmoid', 
                n_clusters=2)
    layer3_regr.fit(X[0:100,:], y[0:100])

    # RMSE score
    np.sqrt(layer3_regr.score(X[100:125,:], y[100:125]))
``
</code></pre>
:ET